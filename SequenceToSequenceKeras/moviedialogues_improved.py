# -*- coding: utf-8 -*-
"""MovieDialogues-Improved.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XGLmtrC3eGmQemnwDRECb4PFVOXWbhBv

## Word level Seq to Seq
Note that this encoder decoder model is a word level encoder decoder model
"""

from keras.models import Model
from keras.preprocessing.text import Tokenizer
from keras.layers import Input, LSTM, Dense, Embedding
from keras.preprocessing.sequence import pad_sequences
import numpy as np

from keras.callbacks import ModelCheckpoint
import nltk
from sklearn.model_selection import train_test_split
from urllib import request
import hashlib

# declaring a random seed before hand for consistency
np.random.seed(42)

batch_size = 64 # declaring the batch size

epochs = 100 # number of epochs to train

glove_embedding_dim = 50 # depends on the dimension of the word embedding, in our case we are using glove

latent_dim = 256

num_samples = 5000

import os, sys

# Here we can have a whitelist set of characters thats the characters that we will use in our chat, other than this we will
# not be using any other characters

def get_hash(token):
    m = hashlib.sha1()
    m.update(token.encode('utf-8'))
    return m.hexdigest()

start_token = "start"
stop_token = "stop"
unknown_token = "unk"
start_hash = get_hash(start_token) + " "
stop_hash = " " + get_hash(stop_token)

print(start_hash)
print(stop_hash)

from google.colab import files
uploaded = files.upload()

!cp a_b_conversations_cleaned.tsv datasets/a_b_conversations_cleaned.tsv

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
 
# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_list = drive.ListFile({'q': "'1hCn8ipd3V75HGwhBdCbwTJh02QzFJTo2' in parents and trashed=false"}).GetList()
for file1 in file_list:
  print('title: %s, id: %s' % (file1['title'], file1['id']))

train_downloaded = drive.CreateFile({'id': '1FB95W6z5uA1zI-WaeMko6j9ba4aoIxHZ'})
train_downloaded.GetContentFile('glove.6B.50d.txt')

!ls datasets

glove_3b_embedding_path = os.path.join("very_large_data", 
                                       "glove.6B." + str(glove_embedding_dim) + "d.txt")
embedding_dict = {}
with open(glove_3b_embedding_path, 'r', encoding='utf-8') as f:
    lines = f.read().split("\n")
    for line in lines:
        tokens = line.split(" ")
        embedding_dict[tokens[0]] = np.array(tokens[1:], dtype=np.float32)
            
vocab = list(embedding_dict.keys())

# here remove the occurance of null string '' from the vocab and the embedding dict
embedding_dict.pop('')
vocab.remove('')

data_path = os.path.join("datasets", 
                         "a_b_conversations_cleaned.tsv") 
# path to the corpus file
# note that it is super important to know the format of the corpus file
# in this case the format is such
# <eng-text><tab><french-translation of the same eng text>
# each new training example is in a new line

input_texts = []
target_texts = []
# input_characters = set()
# target_characters = set()
input_words = set()
target_words = set()

def clean_word(word):
    """
    converts every special character into a separate token
    """
    cleaned_words = []
    temp = ""
    for char in word:
        if (ord(char)>=33 and ord(char)<=47) or (ord(char)>=58 and ord(char)<=64):
#         if (ord(char)>=33 and ord(char)<=47 and ord(char)!=39) or (ord(char)>=58 and ord(char)<=64):
            # separating ordinary characters from special characters, excluding single quote: 39
            if temp != "":
                cleaned_words.append(temp)
                temp = ""
            cleaned_words.append(char)
        else:
            temp += str(char)
            
    if temp != "":
        cleaned_words.append(temp)
    return cleaned_words

def tokenize_target_endpoints(word):
    """
    The target words have '\t' and '\n' as endpoints. These must be considered as separate tokens and used
    """
    tokenized_words = []
    found_cr_t = False
    found_t = False
    found_cr = False
    if start_hash in word and stop_hash in word:
        word = word.replace(start_hash, "")
        word = word.replace(stop_hash, "")
        found_cr_t = True
        
    elif start_hash in word:
        word = word.replace(start_hash, "")
        found_t = True
        
    elif stop_hash in word:
        word = word.replace(stop_hash, "")
        found_cr = True
        
    cleaned_words = clean_word(word)
    tokenized_words.extend(cleaned_words)
    
    if found_cr_t:
        tokenized_words = [start_hash] + tokenized_words + [stop_hash]
    elif found_t:
        tokenized_words = [start_hash] + tokenized_words
    elif found_cr:
        tokenized_words = tokenized_words + [stop_hash]
        
    return tokenized_words

with open(data_path, 'r', encoding='utf-8') as f:
    lines = f.read().split("\n") # spliting training examples

# remember we are trying to get only `sample` number of lines and not any more for training 

input_texts_word2em = []
input_words_per_training_example = []
target_tokens_corpus = []

for line in lines[:min(num_samples, len(lines) - 1)]:
    input_text, target_text = line.split("\t")
    
    target_tokens_linewise = []
    
    target_text = start_hash + target_text + stop_hash
    
    input_text = input_text.lower()
    target_text = target_text.lower()
    
    input_texts.append(input_text)
    target_texts.append(target_text)
    
    input_embeddings_per_training_example = []
    
    for word in input_text.split(" "):
        cleaned_words = clean_word(word)
        for cleaned_word in cleaned_words:
            
            # creating a input_text_word2em list
            if cleaned_word in vocab:
                input_words_per_training_example.append(np.array(embedding_dict[cleaned_word]))
            else:
                # np.zeros is the unknown token vector
                input_words_per_training_example.append(np.zeros(glove_embedding_dim))
                
            if cleaned_word not in input_words:
                input_words.add(cleaned_word)
    #note that the target words need to contain the tab and newline as separate words
    
    input_texts_word2em.append(input_words_per_training_example)
    
    for word in target_text.split(" "):
        tokenized_target_words = tokenize_target_endpoints(word)
        for tokenized_target_word in tokenized_target_words:
            if tokenized_target_word not in target_words:
                target_words.add(tokenized_target_word)
            target_tokens_linewise.append(tokenized_target_word)
                
    target_tokens_corpus.append(target_tokens_linewise)
    
input_words.add(unknown_token)
target_words.add(unknown_token)

input_words = sorted(list(input_words))
target_words = sorted(list(target_words))

num_encoder_tokens = len(input_words)
num_decoder_tokens = len(target_words)

max_encoder_seq_length = 0
for txt in input_texts:
    temp = 0
    for word in txt.split(" "):
        cleaned_words = clean_word(word)
        temp += len(cleaned_words)
    if temp > max_encoder_seq_length:
        max_encoder_seq_length = temp
        
max_decoder_seq_length = 0
for txt in target_texts:
    temp = 0
    for word in txt.split(" "):
        tokenized_target_words = tokenize_target_endpoints(word)
        temp += len(tokenized_target_words)
    if temp > max_decoder_seq_length:
        max_decoder_seq_length = temp
        
print("Number of samples: " + str(len(input_texts))) # total number of samples that we are training
print("Number of unique input tokens: " + str(num_encoder_tokens))
print("Number of unique output tokens: " + str(num_decoder_tokens))
print("Max sequence length for inputs: " + str(max_encoder_seq_length))
print("Max sequence length for outputs: " + str(max_decoder_seq_length))

target_texts

# note that here we are only taking the first 1000 training examples

target_token_index = dict([(word, i) for i, word in enumerate(target_words)])





encoder_inputs = Input(shape=(None, glove_embedding_dim),  
                       name='encoder_inputs') # num_encoder_tokens is essentially the vocab size

# x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)

encoder = LSTM(units=latent_dim, return_state=True, name="encoder")
# Each unit or cell of LSTM within the layer has an internal cell state referred to as c and a hidden state referred
# to as h. 
# If we need to access the internal state c of the last time step we need to use return_state = True

encoder_outputs, state_h, state_c = encoder(encoder_inputs)
# here the encoder_outputs would be hidden state at last time step, state_h would be the hidden state of the last time step
# again and state_c is the internal state of the last time step

encoder_states = [state_h, state_c] # we will only keep the states and discard the output

decoder_inputs = Input(shape=(None, glove_embedding_dim), name="decoder_inputs")

decoder = LSTM(units=latent_dim, return_sequences=True, return_state=True, name="decoder")
# return sequences will return the hidden states for each and every time step

decoder_outputs, decoder_state_h, decoder_state_c = decoder(decoder_inputs, initial_state=encoder_states)
# note here we are passing the encoder_states as the initial_state of the decoder lstm
# decoder_outputs will actially contain the hidden states of all the time steps as an array

decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name="decoder_dense")
# Dense is an ordinary dense neural network.

decoder_outputs = decoder_dense(decoder_outputs)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

def generate_batches(input_word2emmbedding_data, output_text_data):
    """
    input_word2embedding_data: list of lists. where each inner list represents the embedding of each token in one training
    example
    
    output_text_data: list of lists. where each inner list is a set of tokens in one training example
    """
    
          
    num_batches = len(input_word2emmbedding_data) // batch_size # to make it a round number
    while True:
        for index in range(0, num_batches):
            start = index *  batch_size
            end =  (index + 1) * batch_size
            encoder_input_data = pad_sequences(input_word2emmbedding_data[start:end], maxlen=max_encoder_seq_length)
            decoder_input_data = np.zeros(shape=(batch_size, max_decoder_seq_length, glove_embedding_dim))
            decoder_target_data = np.zeros(shape=(batch_size, max_decoder_seq_length, num_decoder_tokens))
            for line_index, target_words in enumerate(output_text_data[start:end]):
                for index, word in enumerate(target_words):
                    w2idx = target_token_index[unknown_token]  # default unknown
                    if word in target_token_index:
                        w2idx = target_token_index[word]
                    if word in embedding_dict:
                        decoder_input_data[line_index, index, :] = embedding_dict[word]
                    if index > 0:
                        decoder_target_data[line_index, index - 1, w2idx] = 1
            yield [encoder_input_data, decoder_input_data], decoder_target_data

!ls models

json = model.to_json()
open('models/word-glove-architecture.json', 'w').write(json)



target_texts_tokens = []
words_in_line = []
for line in target_texts:
  words_in_line = []
  for word in line.split(" "):
    tokenized_target_words = tokenize_target_endpoints(word)
    words_in_line.extend(tokenized_target_words)
  target_texts_tokens.append(words_in_line)
    
# print(target_texts)
# print("----")
# print(target_texts_tokens)
Xtrain, Xtest, Ytrain, Ytest = train_test_split(input_texts_word2em, target_texts_tokens, test_size=0.2, random_state=42)
train_gen = generate_batches(Xtrain, Ytrain)
test_gen = generate_batches(Xtest, Ytest)
print(len(Xtrain))
print(len(Xtest))

#length of each batch
train_num_batches = len(Xtrain) // batch_size
test_num_batches = len(Xtest) // batch_size

WEIGHT_FILE_PATH = os.path.join("models", "word-glove-weights.h5")
checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)

history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,
                    epochs=epochs,
                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches, callbacks=[checkpoint])
# passing all the data here

import matplotlib.pyplot as plt

print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

model.save('s2s2000SamplesWordLevel-Update30-4-18-12-46pm.h5')

# Inference Model (Sampling sequences)
# For sampling the sequences here is the strategy to be followed

# 1. Encode the input and retrieve the initial encoder state
# 2. Run one step of the decoder with its initial encoder state and a "start of sequence" token as target
# 3. Output will be the next target token and current states

# In the original keras example, they are using the training examples themselves to predict the french statements
# However we would like to use predict new statements
# For this we would have to change a few things in order to achieve what we need to achieve

# preparing the test data
test_data_path = os.path.join(os.path.dirname(os.getcwd()), "datasets", "fra-eng", "fra-test.txt") # path to the test 
# corpus file
test_input_texts = []
with open(test_data_path, 'r', encoding='utf-8') as f:
    lines = f.read().split("\n") # spliting training examples

# remember we are trying to get only `sample` number of lines and not any more for training 
for line in lines:
    input_text = line
    
#     target_text = '\t' + target_text + "\n"
    
    test_input_texts.append(input_text)
#     target_texts.append(target_text)
    
encoder_test_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')

for i, (test_input_text) in enumerate(test_input_texts):
    print("i now:" + str(i))
    test_input_text = test_input_text.split(" ")
    for t, word in enumerate(test_input_text):
        # i is the first index as it is the ith training example 
        # t is the second index as the t-th character in ith training example
#         print("t, char now: " + str(t) + ", " + str(char))
    
        if "." in word and word[len(word) - 1] != ".": # patchy method of considering punctuations '.' and ',' as separate words
            period_split_words = word.split(".")
            encoder_test_input_data[i, t, input_token_index["."]] = 1
            encoder_test_input_data[i, t, input_token_index[period_split_words[1]]] = 1
        else:
            encoder_test_input_data[i, t, input_token_index[word]] = 1

encoder_model = Model(encoder_inputs, encoder_states)

decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)

# reverse_input_char_index = dict(
#     (i, char) for char, i in input_token_index.items())
# reverse_target_char_index = dict(
#     (i, char) for char, i in target_token_index.items())
reverse_input_word_index = dict(
    (i, word) for word, i in input_token_index.items())
reverse_target_word_index = dict(
    (i, word) for word, i in target_token_index.items())

def decode_sequence(input_seq):
    """
    This function is for decoding a sequence of english letters
    """
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq) # this is actually encoder_input data

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index['\t']] = 1.
    
    # target_sequence is simply a one hot vector of size vocab size

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        # predicting one character at a time
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = reverse_target_word_index[sampled_token_index]
        decoded_sentence += " " + sampled_word

        # Exit condition: either hit max length
        # or find stop character.
        if (sampled_word == '\n' or len(decoded_sentence) > max_decoder_seq_length):
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.

        # Update states
        states_value = [h, c]

    return decoded_sentence

for seq_index in range(4):
    # this trick below is just for converting input_seq to a list
    input_seq = encoder_test_input_data[seq_index: seq_index + 1] # we will need to form a new encoder_input_data
    decoded_sentence = decode_sequence(input_seq=input_seq)
    print('-')
    print("Input sentence: ", test_input_texts[seq_index])
    print("Decoded sentence: ", decoded_sentence)

