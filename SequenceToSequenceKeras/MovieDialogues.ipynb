{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word level Seq to Seq\n",
    "Note that this encoder decoder model is a word level encoder decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajdeep\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # declaring the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # number of epochs to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                         \"datasets\", \n",
    "                         \"movie-corpus\", \n",
    "                         \"a_b_conversations_cleaned.tsv\") \n",
    "# path to the corpus file\n",
    "# note that it is super important to know the format of the corpus file\n",
    "# in this case the format is such\n",
    "# <eng-text><tab><french-translation of the same eng text>\n",
    "# each new training example is in a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "# input_characters = set()\n",
    "# target_characters = set()\n",
    "input_words = set()\n",
    "target_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    \"\"\"\n",
    "    converts every special character into a separate token except the single quote to avoid converting words like\n",
    "    Don't into 2 separate tokens\n",
    "    \"\"\"\n",
    "    cleaned_words = []\n",
    "    temp = \"\"\n",
    "    for char in word:\n",
    "        if (ord(char)>=33 and ord(char)<=47 and ord(char)!=39) or (ord(char)>=58 and ord(char)<=64):\n",
    "            # separating ordinary characters from special characters, excluding single quote: 39\n",
    "            if temp != \"\":\n",
    "                cleaned_words.append(temp)\n",
    "                temp = \"\"\n",
    "            cleaned_words.append(char)\n",
    "        else:\n",
    "            temp += str(char)\n",
    "            \n",
    "    if temp != \"\":\n",
    "        cleaned_words.append(temp)\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_target_endpoints(word):\n",
    "    \"\"\"\n",
    "    The target words have '\\t' and '\\n' as endpoints. These must be considered as separate tokens and used\n",
    "    \"\"\"\n",
    "    tokenized_words = []\n",
    "    found_cr_t = False\n",
    "    found_t = False\n",
    "    found_cr = False\n",
    "    if \"\\t\" in word and \"\\n\" in word:\n",
    "        word = word.replace(\"\\t\", \"\")\n",
    "        word = word.replace(\"\\n\", \"\")\n",
    "        found_cr_t = True\n",
    "        \n",
    "    elif \"\\t\" in word:\n",
    "        word = word.replace(\"\\t\", \"\")\n",
    "        found_t = True\n",
    "        \n",
    "    elif \"\\n\" in word:\n",
    "        word = word.replace(\"\\n\", \"\")\n",
    "        found_cr = True\n",
    "        \n",
    "    cleaned_words = clean_word(word)\n",
    "    tokenized_words.extend(cleaned_words)\n",
    "    \n",
    "    if found_cr_t:\n",
    "        tokenized_words = [\"\\t\"] + tokenized_words + [\"\\n\"]\n",
    "    elif found_t:\n",
    "        tokenized_words = [\"\\t\"] + tokenized_words\n",
    "    elif found_cr:\n",
    "        tokenized_words = tokenized_words + [\"\\n\"]\n",
    "        \n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of target_texts: 500\n",
      "number of input texts: 500\n"
     ]
    }
   ],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\") # spliting training examples\n",
    "\n",
    "# remember we are trying to get only `sample` number of lines and not any more for training \n",
    "\n",
    "#DEBUG\n",
    "# temp = 0\n",
    "for line in lines[:min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split(\"\\t\")\n",
    "    \n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "#     print(\"input_text: \" + str(input_text))\n",
    "#     print(\"target_text: \" + str(target_text))\n",
    "#     if temp > 4:\n",
    "#         break\n",
    "        \n",
    "#     temp +=1\n",
    "    \n",
    "    # including chars in input and target characters\n",
    "#     for char in input_text:\n",
    "#         if char not in input_characters:\n",
    "#             input_characters.add(char)\n",
    "    \n",
    "#     for char in target_text:\n",
    "#         if char not in target_characters:\n",
    "#             target_characters.add(char)\n",
    "    for word in input_text.split(\" \"):\n",
    "        cleaned_words = clean_word(word)\n",
    "        for cleaned_word in cleaned_words:\n",
    "            if cleaned_word not in input_words:\n",
    "                input_words.add(cleaned_word)\n",
    "    #note that the target words need to contain the tab and newline as separate words\n",
    "    \n",
    "    for word in target_text.split(\" \"):\n",
    "        tokenized_target_words = tokenize_target_endpoints(word)\n",
    "        for tokenized_target_word in tokenized_target_words:\n",
    "            if tokenized_target_word not in target_words:\n",
    "                target_words.add(tokenized_target_word)\n",
    "\n",
    "print(\"number of target_texts: \" + str(len(target_texts)))\n",
    "print(\"number of input texts: \" + str(len(input_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 500\n",
      "Number of unique input tokens: 1425\n",
      "Number of unique output tokens: 1250\n",
      "Max sequence length for inputs: 66\n",
      "Max sequence length for outputs: 106\n"
     ]
    }
   ],
   "source": [
    "# here input_characters is the total number of input characters\n",
    "# input_characters = sorted(list(input_characters))\n",
    "# target_characters = sorted(list(target_characters))\n",
    "input_words = sorted(list(input_words))\n",
    "target_words = sorted(list(target_words))\n",
    "\n",
    "# num_encoder_tokens = len(input_characters)\n",
    "# num_decoder_tokens = len(target_characters)\n",
    "num_encoder_tokens = len(input_words)\n",
    "num_decoder_tokens = len(target_words)\n",
    "\n",
    "\n",
    "# max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "# max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "max_encoder_seq_length = 0\n",
    "for txt in input_texts:\n",
    "    temp = 0\n",
    "    for word in txt.split(\" \"):\n",
    "        cleaned_words = clean_word(word)\n",
    "        temp += len(cleaned_words)\n",
    "    if temp > max_encoder_seq_length:\n",
    "        max_encoder_seq_length = temp\n",
    "        \n",
    "max_decoder_seq_length = 0\n",
    "for txt in target_texts:\n",
    "    temp = 0\n",
    "    for word in txt.split(\" \"):\n",
    "        tokenized_target_words = tokenize_target_endpoints(word)\n",
    "        temp += len(tokenized_target_words)\n",
    "    if temp > max_decoder_seq_length:\n",
    "        max_decoder_seq_length = temp\n",
    "        \n",
    "print(\"Number of samples: \" + str(len(input_texts))) # total number of samples that we are training\n",
    "print(\"Number of unique input tokens: \" + str(num_encoder_tokens))\n",
    "print(\"Number of unique output tokens: \" + str(num_decoder_tokens))\n",
    "print(\"Max sequence length for inputs: \" + str(max_encoder_seq_length))\n",
    "print(\"Max sequence length for outputs: \" + str(max_decoder_seq_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that here we are only taking the first 1000 training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "input_token_index = dict([(word, i) for i, word in enumerate(input_words)])\n",
    "# recall that input_characters is a list of all the unique input characters that we have in the 1000 training examples\n",
    "# that we have read so far\n",
    "\n",
    "# what enumerate does is that it assigns a counter to the unique characters, so basically if we get the following list\n",
    "# a = [\"ola\", \"mundo\", \"keras\"], we would get back from enumerate 1, \"ola\"; 2, \"mundo\"; 3, \"keras\"\n",
    "\n",
    "# target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "target_token_index = dict([(word, i) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the important part as we are defining what goes into the input data\n",
    "# max_encoder seq length is the maximum number of characters in one training example in the entire corpus\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "# the number of input_texts and the number of target_texts will be the same\n",
    "decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "# here we just created the vectors that we are going to use as inputs for the encoder and inputs and targets for the \n",
    "# decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING CODE\n",
    "# temp = 0\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "#     temp += 1\n",
    "#     if temp>4:\n",
    "#         break\n",
    "    \n",
    "    cleaned_words = []\n",
    "    for split_word in input_text.split(\" \"):\n",
    "        cleaned_words.extend(clean_word(split_word)) \n",
    "        \n",
    "#     print(\"cleaned_words: \" , end=\"\")\n",
    "#     print(cleaned_words)\n",
    "    \n",
    "    tokenized_target_words = []\n",
    "    for split_word in target_text.split(\" \"):\n",
    "        tokenized_target_words.extend(tokenize_target_endpoints(split_word))\n",
    "        \n",
    "#     print(\"tokenized_target_words: \" , end=\"\")\n",
    "#     print(tokenized_target_words)\n",
    "    \n",
    "    for t, word in enumerate(cleaned_words):\n",
    "        # i is the first index as it is the ith training example \n",
    "        # t is the second index as the t-th character in ith training example\n",
    "        encoder_input_data[i, t, input_token_index[word]] = 1\n",
    "        \n",
    "#     print(\"encoder_input_data: \" , end=\"\")\n",
    "#     print(encoder_input_data[i])\n",
    "            \n",
    "    for t, word in enumerate(tokenized_target_words):\n",
    "        # here a tb splitting must be done in order to treat the tab as a separate word\n",
    "        decoder_input_data[i, t, target_token_index[word]] = 1\n",
    "            \n",
    "        # Here is an important thing to understand\n",
    "        ###############---------------------------IMPORTANT------------------------#####################\n",
    "        # The decoder_target_data will be ahead of the decoder_input_data by one time step and this is an important factor\n",
    "        # The decoder_target_data will obviously not contain the first character\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t-1, target_token_index[word]] = 1\n",
    "    \n",
    "#     print(\"decoder_input_data: \" , end=\"\")\n",
    "#     print(decoder_input_data[i])\n",
    "    \n",
    "#     print(\"decoder_target_data: \" , end=\"\")\n",
    "#     print(decoder_target_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "print(len(target_token_index.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens)) # num_encoder_tokens is essentially the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "# Each unit or cell of LSTM within the layer has an internal cell state referred to as c and a hidden state referred\n",
    "# to as h. \n",
    "# If we need to access the internal state c of the last time step we need to use return_state = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# here the encoder_outputs would be hidden state at last time step, state_h would be the hidden state of the last time step\n",
    "# again and state_c is the internal state of the last time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_states = [state_h, state_c] # we will only keep the states and discard the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "# return sequences will return the hidden states for each and every time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# note here we are passing the encoder_states as the initial_state of the decoder lstm\n",
    "# decoder_outputs will actially contain the hidden states of all the time steps as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "# Dense is an ordinary dense neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.6278 - acc: 0.0318 - val_loss: 0.6411 - val_acc: 0.0154\n",
      "Epoch 2/5\n",
      "128/400 [========>.....................] - ETA: 24s - loss: 0.5092 - acc: 0.0094"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-ba60c7a53fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           validation_split=0.2)\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# passing all the data here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_split=0.2)\n",
    "# passing all the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajdeep\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer lstm_5 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_4/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_4/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('s2s2000SamplesWordLevel-Update30-4-18-12-46pm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Model (Sampling sequences)\n",
    "# For sampling the sequences here is the strategy to be followed\n",
    "\n",
    "# 1. Encode the input and retrieve the initial encoder state\n",
    "# 2. Run one step of the decoder with its initial encoder state and a \"start of sequence\" token as target\n",
    "# 3. Output will be the next target token and current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the original keras example, they are using the training examples themselves to predict the french statements\n",
    "# However we would like to use predict new statements\n",
    "# For this we would have to change a few things in order to achieve what we need to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i now:0\n",
      "i now:1\n",
      "i now:2\n",
      "i now:3\n"
     ]
    }
   ],
   "source": [
    "# preparing the test data\n",
    "test_data_path = os.path.join(os.path.dirname(os.getcwd()), \"datasets\", \"fra-eng\", \"fra-test.txt\") # path to the test \n",
    "# corpus file\n",
    "test_input_texts = []\n",
    "with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\") # spliting training examples\n",
    "\n",
    "# remember we are trying to get only `sample` number of lines and not any more for training \n",
    "for line in lines:\n",
    "    input_text = line\n",
    "    \n",
    "#     target_text = '\\t' + target_text + \"\\n\"\n",
    "    \n",
    "    test_input_texts.append(input_text)\n",
    "#     target_texts.append(target_text)\n",
    "    \n",
    "encoder_test_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for i, (test_input_text) in enumerate(test_input_texts):\n",
    "    print(\"i now:\" + str(i))\n",
    "    test_input_text = test_input_text.split(\" \")\n",
    "    for t, word in enumerate(test_input_text):\n",
    "        # i is the first index as it is the ith training example \n",
    "        # t is the second index as the t-th character in ith training example\n",
    "#         print(\"t, char now: \" + str(t) + \", \" + str(char))\n",
    "    \n",
    "        if \".\" in word and word[len(word) - 1] != \".\": # patchy method of considering punctuations '.' and ',' as separate words\n",
    "            period_split_words = word.split(\".\")\n",
    "            encoder_test_input_data[i, t, input_token_index[\".\"]] = 1\n",
    "            encoder_test_input_data[i, t, input_token_index[period_split_words[1]]] = 1\n",
    "        else:\n",
    "            encoder_test_input_data[i, t, input_token_index[word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_input_char_index = dict(\n",
    "#     (i, char) for char, i in input_token_index.items())\n",
    "# reverse_target_char_index = dict(\n",
    "#     (i, char) for char, i in target_token_index.items())\n",
    "reverse_input_word_index = dict(\n",
    "    (i, word) for word, i in input_token_index.items())\n",
    "reverse_target_word_index = dict(\n",
    "    (i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \"\"\"\n",
    "    This function is for decoding a sequence of english letters\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq) # this is actually encoder_input data\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    \n",
    "    # target_sequence is simply a one hot vector of size vocab size\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        # predicting one character at a time\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_word_index[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence:  He must go.\n",
      "Decoded sentence:   nous faut\n",
      "-\n",
      "Input sentence:  He had loved tea.\n",
      "Decoded sentence:   a le tripes.\n",
      "\n",
      "-\n",
      "Input sentence:  I met Tom.\n",
      "Decoded sentence:   me suis construit.\n",
      "\n",
      "-\n",
      "Input sentence:  Tom is cute!\n",
      "Decoded sentence:   est nouveau.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(4):\n",
    "    # this trick below is just for converting input_seq to a list\n",
    "    input_seq = encoder_test_input_data[seq_index: seq_index + 1] # we will need to form a new encoder_input_data\n",
    "    decoded_sentence = decode_sequence(input_seq=input_seq)\n",
    "    print('-')\n",
    "    print(\"Input sentence: \", test_input_texts[seq_index])\n",
    "    print(\"Decoded sentence: \", decoded_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
